{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.图片预处理，洗数据\n",
    "2.将train每个图片路径以列表形式保存，分成训练集和测试集。\n",
    "3.创建tfrecord。分别将训练集和测试集的图片放入tfrecord中，得到训练tfrecord和测试tfrecord。\n",
    "4.建立模型。三卷积两全连接加最后一层输出层。\n",
    "5.训练模型。从tfrecord采用minibatch方式读取训练。\n",
    "6.模型保存。\n",
    "7.模型读取，进行单个图片的识别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练图片全部在一个文件夹train，再分别放到train1的两个cat和dog文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import tensorflow as tf  \n",
    "from PIL import Image\n",
    "# os.getcwd()\n",
    "# os.makedirs('train1/train_cat')\n",
    "# os.makedirs('train1/train_dog')\n",
    "\n",
    "import os  \n",
    "import tensorflow as tf  \n",
    "import shutil,os\n",
    "from PIL import Image\n",
    "\n",
    "for img_name in os.listdir('train'):\n",
    "    if(img_name[0:3]=='cat'):\n",
    "        shutil.move('train/'+img_name,'train1/train_cat')\n",
    "for img_name in os.listdir('train'):\n",
    "    if(img_name[0:3]=='dog'):\n",
    "        shutil.move('train/'+img_name,'train1/train_dog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将数据路径放到一个list，打乱顺序，分为训练集和测试集，建立两个tfrecord "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenameTrain = 'dog_cat_train.tfrecords'\n",
    "filenameTest = 'dog_cat_test.tfrecords'\n",
    "orig_picture = 'train1'\n",
    "#数据目录 train1/dog/,train1/cat\n",
    "random.seed(2)\n",
    "tfresizeW = 128#resize图片大小\n",
    "tfresizeH = 128\n",
    "def creat_tfrecord(test_ratio):\n",
    "    totalimg = list()\n",
    "    totallabel = list()\n",
    "    writerTrain = tf.python_io.TFRecordWriter(filenameTrain)\n",
    "    writerTest = tf.python_io.TFRecordWriter(filenameTest)\n",
    "    for index,name in enumerate(classes):\n",
    "        class_path = orig_picture +\"/\"+ name+\"/\" \n",
    "        for img_name in os.listdir(class_path):\n",
    "            img_path = class_path + img_name\n",
    "            totalimg.append(img_path)\n",
    "            totallabel.append(index)\n",
    "    zipList = list(zip(totalimg,totallabel))\n",
    "    random.shuffle(zipList)\n",
    "    totalimg[:],totallabel[:] = zip(*zipList)\n",
    "    i = 0\n",
    "    for imgPath,imgLable in zip(totalimg,totallabel):\n",
    "        img = Image.open(imgPath)\n",
    "        img = img.resize((tfresizeW,tfresizeH))\n",
    "        img_raw = img.tobytes()\n",
    "        example = tf.train.Example(  #将数据写入到数据缓冲区\n",
    "           features=tf.train.Features(feature={  \n",
    "                \"label\": tf.train.Feature(int64_list=tf.train.Int64List(value=[imgLable])),  \n",
    "                'img_raw': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_raw]))  \n",
    "           }))\n",
    "        if i<=len(totalimg) * test_ratio:\n",
    "            writerTest.write(example.SerializeToString())\n",
    "        else:\n",
    "            writerTrain.write(example.SerializeToString())\n",
    "        i += 1\n",
    "    writerTrain.close()\n",
    "    writerTest.close()\n",
    "    \n",
    "creat_tfrecord(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import tensorflow as tf  \n",
    "from PIL import Image\n",
    "# os.getcwd()\n",
    "# os.makedirs('train/train_cat')\n",
    "# os.makedirs('train/train_dog')\n",
    "\n",
    "import os  \n",
    "import tensorflow as tf  \n",
    "import shutil,os\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读取tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_decode(filename):  #读取队列，先进先出，读取一个之后，后面自动补齐\n",
    "    # 创建文件队列,不限读取的数量  \n",
    "    filename_queue = tf.train.string_input_producer([filename]) #读取解析队列 \n",
    "    # create a reader from file queue  \n",
    "    reader = tf.TFRecordReader()  \n",
    "    # reader从文件队列中读入一个序列化的样本  \n",
    "    _, serialized_example = reader.read(filename_queue)  #返回文件名和文件\n",
    "    # get feature from serialized example  \n",
    "    # 解析符号化的样本  \n",
    "    features = tf.parse_single_example(  #将example协议块解析为张量\n",
    "        serialized_example,  \n",
    "        features={  \n",
    "            'label': tf.FixedLenFeature([], tf.int64),  \n",
    "            'img_raw': tf.FixedLenFeature([], tf.string)  \n",
    "        })  \n",
    "    label = features['label']  \n",
    "    img = features['img_raw']  \n",
    "    img = tf.decode_raw(img, tf.uint8)  #将to_bytes的字符串格式转成uint8\n",
    "    img = tf.reshape(img, [128, 128, 3])  \n",
    "#     img = tf.cast(img, tf.float32) *(1./255) - 0.5\n",
    "    img = tf.image.per_image_standardization(img)\n",
    "    label = tf.cast(label, tf.int32)  \n",
    "    return img, label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CHANNELS = 3\n",
    "\n",
    "CONV1_DEEP = 32\n",
    "CONV1_SIZE = 3\n",
    "\n",
    "CONV2_DEEP = 64\n",
    "CONV2_SIZE = 3\n",
    "\n",
    "FC_SIZE1 = 128\n",
    "FC_SIZE2 = 256\n",
    "\n",
    "REGULARIZATION_RATE = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#写网络用于训练，并存储网络\n",
    "#=========================================================================\n",
    "import tensorflow as tf\n",
    "#=========================================================================\n",
    "#网络结构定义\n",
    "    #输入参数：images，image batch、4D tensor、tf.float32、[batch_size, width, height, channels]\n",
    "    #返回参数：logits, float、 [batch_size, n_classes]\n",
    "def inference(images, batch_size, n_classes,train,regularizer_num):\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "#一个简单的卷积神经网络，卷积+池化层x2，全连接层x2，最后一个softmax层做分类。\n",
    "#卷积层1\n",
    "#64个3x3的卷积核（3通道），padding=’SAME’，表示padding后卷积的图与原图尺寸一致，激活函数relu()\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "\n",
    "        weights1 = tf.Variable(tf.truncated_normal(shape=[CONV1_SIZE,CONV1_SIZE,NUM_CHANNELS,CONV1_DEEP], \n",
    "                                                   stddev = 0.01, dtype = tf.float32), \n",
    "                              name = 'weights', dtype = tf.float32)\n",
    "\n",
    "        biases1 = tf.Variable(tf.constant(value = 0, dtype = tf.float32, shape = [CONV1_DEEP]),\n",
    "                             name = 'biases', dtype = tf.float32)\n",
    "\n",
    "        conv1 = tf.nn.conv2d(images, weights1, strides=[1,1,1,1], padding='SAME')\n",
    "        pre_activation = tf.nn.bias_add(conv1, biases1)\n",
    "        conv11 = tf.nn.relu(pre_activation, name= scope.name)\n",
    "\n",
    "#池化层1\n",
    "#3x3最大池化，步长strides为2，池化后执行lrn()操作，局部响应归一化，对训练有利。\n",
    "    with tf.variable_scope('pooling1_lrn') as scope:\n",
    "        pool1 = tf.nn.max_pool(conv11, ksize=[1,3,3,1],strides=[1,2,2,1],padding='SAME', name='pooling1')\n",
    "        norm1 = tf.nn.lrn(pool1, depth_radius=4, bias=1.0, alpha=0.001/9.0, beta=0.75, name='norm1')\n",
    "        norm1 =pool1\n",
    "#卷积层2\n",
    "#16个3x3的卷积核（16通道），padding=’SAME’，表示padding后卷积的图与原图尺寸一致，激活函数relu()\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        weights2 = tf.Variable(tf.truncated_normal(shape=[CONV2_SIZE,CONV2_SIZE,CONV1_DEEP,CONV2_DEEP], \n",
    "                                                   stddev = 0.01, dtype = tf.float32), \n",
    "                              name = 'weights', dtype = tf.float32)\n",
    "\n",
    "        biases2 = tf.Variable(tf.constant(value = 0, dtype = tf.float32, shape = [CONV2_DEEP]),\n",
    "                             name = 'biases', dtype = tf.float32)\n",
    "\n",
    "        conv2 = tf.nn.conv2d(norm1, weights2, strides = [1,1,1,1],padding='SAME')\n",
    "        pre_activation = tf.nn.bias_add(conv2, biases2)\n",
    "        conv22 = tf.nn.relu(pre_activation, name='conv2')\n",
    " #池化层2\n",
    "#3x3最大池化，步长strides为2，池化后执行lrn()操作，\n",
    "    #pool2 and norm2\n",
    "    with tf.variable_scope('pooling2_lrn') as scope:\n",
    "        pool2 = tf.nn.max_pool(conv22, ksize=[1,3,3,1], strides=[1,1,1,1],padding='SAME',name='pooling2')\n",
    "        norm2 = tf.nn.lrn(pool2, depth_radius=4, bias=1.0, alpha=0.001/9.0,beta=0.75,name='norm2')\n",
    "        norm2 =pool2\n",
    "# #卷积层3\n",
    "# #16个3x3的卷积核（16通道），padding=’SAME’，表示padding后卷积的图与原图尺寸一致，激活函数relu()\n",
    "#     with tf.variable_scope('conv3') as scope:\n",
    "#         weights5 = tf.Variable(tf.truncated_normal(shape=[CONV3_SIZE,CONV3_SIZE,CONV2_DEEP,CONV3_DEEP], \n",
    "#                                                    stddev = 0.01, dtype = tf.float32), \n",
    "#                               name = 'weights', dtype = tf.float32)\n",
    "\n",
    "#         biases5 = tf.Variable(tf.constant(value = 0, dtype = tf.float32, shape = [CONV3_DEEP]),\n",
    "#                              name = 'biases', dtype = tf.float32)\n",
    "\n",
    "#         conv = tf.nn.conv2d(norm2, weights5, strides = [1,1,1,1],padding='SAME')\n",
    "#         pre_activation = tf.nn.bias_add(conv, biases5)\n",
    "#         conv5 = tf.nn.relu(pre_activation, name='conv5')\n",
    "#  #池化层3\n",
    "# #3x3最大池化，步长strides为2，池化后执行lrn()操作，\n",
    "#     #pool2 and norm2\n",
    "#     with tf.variable_scope('pooling5_lrn') as scope:\n",
    "#         pool5 = tf.nn.max_pool(conv5, ksize=[1,3,3,1], strides=[1,1,1,1],padding='SAME',name='pooling5')\n",
    "#         norm5 = tf.nn.lrn(pool5, depth_radius=4, bias=1.0, alpha=0.001/9.0,beta=0.75,name='norm5')\n",
    "\n",
    "    \n",
    "#全连接层3\n",
    "#128个神经元，将之前pool层的输出reshape成一行，激活函数relu()\n",
    "    with tf.variable_scope('local3') as scope:\n",
    "        reshape = tf.reshape(norm2, shape=[batch_size, -1])\n",
    "        dim = reshape.get_shape()[1].value\n",
    "        weights3 = tf.Variable(tf.truncated_normal(shape=[dim,FC_SIZE1], stddev = 0.01, dtype = tf.float32),\n",
    "                             name = 'weights', dtype = tf.float32)\n",
    "        wloss_w3 = regularizer(weights3)\n",
    "        if regularizer_num is not None:\n",
    "            tf.add_to_collection('losses', wloss_w3)\n",
    "        biases3 = tf.Variable(tf.constant(value = 0, dtype = tf.float32, shape = [FC_SIZE1]), \n",
    "                             name = 'biases', dtype=tf.float32)\n",
    "\n",
    "        local3 = tf.nn.relu(tf.matmul(reshape, weights3) + biases3, name=scope.name)\n",
    "        #dropout层        \n",
    "#     with tf.variable_scope('dropout') as scope:\n",
    "#         if train is not None:\n",
    "#             local3 = tf.nn.dropout(local3, 0.5)\n",
    "\n",
    "#全连接层4\n",
    "#128个神经元，激活函数relu() \n",
    "    with tf.variable_scope('local4') as scope:\n",
    "        weights4 = tf.Variable(tf.truncated_normal(shape=[FC_SIZE1,FC_SIZE2], stddev = 0.01, \n",
    "                                                   dtype = tf.float32),name = 'weights',dtype = tf.float32)\n",
    "        wloss_w4 = regularizer(weights4)\n",
    "        if regularizer_num is not None:\n",
    "            tf.add_to_collection('losses', wloss_w4)\n",
    "        biases4 = tf.Variable(tf.constant(value = 0, dtype = tf.float32, shape = [FC_SIZE2]),\n",
    "                             name = 'biases', dtype = tf.float32)\n",
    "\n",
    "        local4 = tf.nn.relu(tf.matmul(local3, weights4) + biases4, name='local4')\n",
    "#dropout层        \n",
    "    with tf.variable_scope('dropout') as scope:\n",
    "        if train is not None:\n",
    "            local4 = tf.nn.dropout(local4, 0.9)\n",
    "\n",
    "\n",
    "#Softmax回归层\n",
    "#将前面的FC层输出，做一个线性回归，计算出每一类的得分，在这里是2类，所以这个层输出的是两个得分。\n",
    "    with tf.variable_scope('softmax_linear') as scope:\n",
    "        weights5 = tf.Variable(tf.truncated_normal(shape=[FC_SIZE2, n_classes], stddev = 0.01, \n",
    "                                                   dtype = tf.float32),name = 'softmax_linear', dtype = tf.float32)\n",
    "\n",
    "        biases5 = tf.Variable(tf.constant(value = 0, dtype = tf.float32, shape = [n_classes]),\n",
    "                             name = 'biases', dtype = tf.float32)\n",
    "\n",
    "        softmax_linear = tf.add(tf.matmul(local4, weights5), biases5, name='softmax_linear')\n",
    "#         softmax_linear = tf.nn.softmax(softmax_linear)\n",
    "    return softmax_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def losses(logits, labels,flag):\n",
    "    with tf.variable_scope('loss') as scope:\n",
    "        cross_entropy =tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels, name='xentropy_per_example')\n",
    "        loss = tf.reduce_mean(cross_entropy, name='loss')\n",
    "        tf.add_to_collection('losses', loss)\n",
    "        if flag==1:\n",
    "            all_loss = tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "        else:\n",
    "            all_loss = loss\n",
    "        tf.summary.scalar(scope.name+'/loss', all_loss)\n",
    "    return all_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainning(loss, learning_rate):\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate= learning_rate)\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        train_op = optimizer.minimize(loss, global_step= global_step)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(logits, labels):\n",
    "    with tf.variable_scope('accuracy') as scope:\n",
    "        correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "        correct = tf.cast(correct, tf.float16)\n",
    "        accuracy = tf.reduce_mean(correct)\n",
    "        tf.summary.scalar(scope.name+'/accuracy', accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#变量声明\n",
    "N_CLASSES = 2  #分类数量\n",
    "IMG_W = 128   # resize图像，太大的话训练时间久\n",
    "IMG_H = 128\n",
    "CHANNEL = 3\n",
    "BATCH_SIZE =32#训练的mini_batch大小\n",
    "CAPACITY = 2000#队列存放数量\n",
    "MAX_STEP = 10000 # 最大训练步骤\n",
    "learning_rate = 0.0001# 一般小于0.0001初始学习率\n",
    "logs_train_dir = 'log'    #logs存储路径"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将train和test分开，但共用一个网络结构，不能建立新的test否则输入重新生成一个分支，不能使用最新的参数。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_training = tf.placeholder(tf.bool, shape=())\n",
    "\n",
    "train_batch = tf.placeholder(tf.float32,shape=(BATCH_SIZE,IMG_W,IMG_H,CHANNEL))\n",
    "train_label_batch = tf.placeholder(tf.int64,shape=(BATCH_SIZE))\n",
    "\n",
    "val_batch = tf.placeholder(tf.float32,shape=(BATCH_SIZE,IMG_W,IMG_H,CHANNEL))\n",
    "val_label_batch = tf.placeholder(tf.int64,shape=(BATCH_SIZE))\n",
    "\n",
    "x = tf.cond(is_training, lambda: train_batch, lambda: val_batch)\n",
    "y = tf.cond(is_training, lambda: train_label_batch, lambda: val_label_batch)\n",
    "isdrop = tf.cond(is_training, lambda: True, lambda: False)\n",
    "isl2 = tf.cond(is_training, lambda: True, lambda: False)\n",
    "\n",
    "logits = inference(x,BATCH_SIZE,N_CLASSES,isdrop,isl2)\n",
    "loss = losses(logits,y,isl2)\n",
    "train_op = trainning(loss,learning_rate)\n",
    "acc = evaluation(logits, y)\n",
    "\n",
    "\n",
    "#这个是log汇总记录\n",
    "summary_op = tf.summary.merge_all() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img,train_label = read_and_decode('dog_cat_train.tfrecords')\n",
    "test_img,test_label = read_and_decode('dog_cat_test.tfrecords')\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_batch, label_batch = tf.train.shuffle_batch([train_img, train_label],\n",
    "                                                batch_size=BATCH_SIZE, capacity=CAPACITY,\n",
    "                                                min_after_dequeue=1000)\n",
    "img_batch_test,label_batch_test = tf.train.shuffle_batch([test_img, test_label],\n",
    "                                                batch_size=BATCH_SIZE, capacity=CAPACITY,\n",
    "                                                min_after_dequeue=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, train loss = 0.69, train accuracy = 56.25%\n",
      "Step 0, test loss = 0.69, test accuracy = 50.00%\n",
      "Step 50, train loss = 0.68, train accuracy = 53.12%\n",
      "Step 50, test loss = 0.68, test accuracy = 50.00%\n",
      "Step 100, train loss = 0.67, train accuracy = 62.50%\n",
      "Step 100, test loss = 0.68, test accuracy = 50.00%\n",
      "Step 150, train loss = 0.60, train accuracy = 65.62%\n",
      "Step 150, test loss = 0.58, test accuracy = 81.25%\n",
      "Step 200, train loss = 0.54, train accuracy = 68.75%\n",
      "Step 200, test loss = 0.50, test accuracy = 78.12%\n",
      "Step 250, train loss = 0.64, train accuracy = 65.62%\n",
      "Step 250, test loss = 0.57, test accuracy = 65.62%\n",
      "Step 300, train loss = 0.64, train accuracy = 68.75%\n",
      "Step 300, test loss = 0.61, test accuracy = 56.25%\n",
      "Step 350, train loss = 0.62, train accuracy = 62.50%\n",
      "Step 350, test loss = 0.52, test accuracy = 81.25%\n",
      "Step 400, train loss = 0.63, train accuracy = 62.50%\n",
      "Step 400, test loss = 0.63, test accuracy = 53.12%\n",
      "Step 450, train loss = 0.47, train accuracy = 71.88%\n",
      "Step 450, test loss = 0.48, test accuracy = 78.12%\n",
      "Step 500, train loss = 0.59, train accuracy = 62.50%\n",
      "Step 500, test loss = 0.42, test accuracy = 84.38%\n",
      "Step 550, train loss = 0.51, train accuracy = 68.75%\n",
      "Step 550, test loss = 0.52, test accuracy = 71.88%\n",
      "Step 600, train loss = 0.64, train accuracy = 59.38%\n",
      "Step 600, test loss = 0.59, test accuracy = 75.00%\n",
      "Step 650, train loss = 0.71, train accuracy = 62.50%\n",
      "Step 650, test loss = 0.48, test accuracy = 78.12%\n",
      "Step 700, train loss = 0.50, train accuracy = 75.00%\n",
      "Step 700, test loss = 0.45, test accuracy = 84.38%\n",
      "Step 750, train loss = 0.73, train accuracy = 68.75%\n",
      "Step 750, test loss = 0.51, test accuracy = 84.38%\n",
      "Step 800, train loss = 0.45, train accuracy = 87.50%\n",
      "Step 800, test loss = 0.40, test accuracy = 84.38%\n",
      "Step 850, train loss = 0.43, train accuracy = 75.00%\n",
      "Step 850, test loss = 0.61, test accuracy = 62.50%\n",
      "Step 900, train loss = 0.44, train accuracy = 84.38%\n",
      "Step 900, test loss = 0.42, test accuracy = 81.25%\n",
      "Step 950, train loss = 0.59, train accuracy = 71.88%\n",
      "Step 950, test loss = 0.42, test accuracy = 81.25%\n",
      "Step 1000, train loss = 0.40, train accuracy = 81.25%\n",
      "Step 1000, test loss = 0.43, test accuracy = 75.00%\n",
      "Step 1050, train loss = 0.51, train accuracy = 71.88%\n",
      "Step 1050, test loss = 0.43, test accuracy = 78.12%\n",
      "Step 1100, train loss = 0.51, train accuracy = 68.75%\n",
      "Step 1100, test loss = 0.74, test accuracy = 65.62%\n",
      "Step 1150, train loss = 0.53, train accuracy = 75.00%\n",
      "Step 1150, test loss = 0.50, test accuracy = 81.25%\n",
      "Step 1200, train loss = 0.43, train accuracy = 84.38%\n",
      "Step 1200, test loss = 0.77, test accuracy = 62.50%\n",
      "Step 1250, train loss = 0.45, train accuracy = 75.00%\n",
      "Step 1250, test loss = 0.52, test accuracy = 68.75%\n",
      "Step 1300, train loss = 0.35, train accuracy = 87.50%\n",
      "Step 1300, test loss = 0.49, test accuracy = 75.00%\n",
      "Step 1350, train loss = 0.71, train accuracy = 65.62%\n",
      "Step 1350, test loss = 0.50, test accuracy = 71.88%\n",
      "Step 1400, train loss = 0.36, train accuracy = 87.50%\n",
      "Step 1400, test loss = 0.36, test accuracy = 87.50%\n",
      "Step 1450, train loss = 0.55, train accuracy = 75.00%\n",
      "Step 1450, test loss = 0.60, test accuracy = 68.75%\n",
      "Step 1500, train loss = 0.72, train accuracy = 62.50%\n",
      "Step 1500, test loss = 0.59, test accuracy = 71.88%\n",
      "Step 1550, train loss = 0.32, train accuracy = 87.50%\n",
      "Step 1550, test loss = 0.35, test accuracy = 87.50%\n",
      "Step 1600, train loss = 0.48, train accuracy = 71.88%\n",
      "Step 1600, test loss = 0.36, test accuracy = 84.38%\n",
      "Step 1650, train loss = 0.45, train accuracy = 78.12%\n",
      "Step 1650, test loss = 0.43, test accuracy = 81.25%\n",
      "Step 1700, train loss = 0.47, train accuracy = 81.25%\n",
      "Step 1700, test loss = 0.41, test accuracy = 78.12%\n",
      "Step 1750, train loss = 0.43, train accuracy = 78.12%\n",
      "Step 1750, test loss = 0.38, test accuracy = 81.25%\n",
      "Step 1800, train loss = 0.42, train accuracy = 71.88%\n",
      "Step 1800, test loss = 0.43, test accuracy = 84.38%\n",
      "Step 1850, train loss = 0.47, train accuracy = 84.38%\n",
      "Step 1850, test loss = 0.47, test accuracy = 81.25%\n",
      "Step 1900, train loss = 0.35, train accuracy = 84.38%\n",
      "Step 1900, test loss = 0.51, test accuracy = 75.00%\n",
      "Step 1950, train loss = 0.44, train accuracy = 78.12%\n",
      "Step 1950, test loss = 0.42, test accuracy = 84.38%\n",
      "Step 2000, train loss = 0.23, train accuracy = 100.00%\n",
      "Step 2000, test loss = 0.34, test accuracy = 90.62%\n",
      "Step 2050, train loss = 0.26, train accuracy = 90.62%\n",
      "Step 2050, test loss = 0.33, test accuracy = 87.50%\n",
      "Step 2100, train loss = 0.45, train accuracy = 78.12%\n",
      "Step 2100, test loss = 0.49, test accuracy = 78.12%\n",
      "Step 2150, train loss = 0.47, train accuracy = 75.00%\n",
      "Step 2150, test loss = 0.71, test accuracy = 62.50%\n",
      "Step 2200, train loss = 0.44, train accuracy = 78.12%\n",
      "Step 2200, test loss = 0.52, test accuracy = 75.00%\n",
      "Step 2250, train loss = 0.45, train accuracy = 75.00%\n",
      "Step 2250, test loss = 0.50, test accuracy = 78.12%\n",
      "Step 2300, train loss = 0.41, train accuracy = 81.25%\n",
      "Step 2300, test loss = 0.44, test accuracy = 75.00%\n",
      "Step 2350, train loss = 0.33, train accuracy = 84.38%\n",
      "Step 2350, test loss = 0.31, test accuracy = 87.50%\n",
      "Step 2400, train loss = 0.50, train accuracy = 71.88%\n",
      "Step 2400, test loss = 0.45, test accuracy = 81.25%\n",
      "Step 2450, train loss = 0.57, train accuracy = 75.00%\n",
      "Step 2450, test loss = 0.27, test accuracy = 87.50%\n",
      "Step 2500, train loss = 0.43, train accuracy = 84.38%\n",
      "Step 2500, test loss = 0.60, test accuracy = 68.75%\n",
      "Step 2550, train loss = 0.21, train accuracy = 90.62%\n",
      "Step 2550, test loss = 0.29, test accuracy = 87.50%\n",
      "Step 2600, train loss = 0.36, train accuracy = 81.25%\n",
      "Step 2600, test loss = 0.23, test accuracy = 93.75%\n",
      "Step 2650, train loss = 0.36, train accuracy = 87.50%\n",
      "Step 2650, test loss = 0.33, test accuracy = 87.50%\n",
      "Step 2700, train loss = 0.26, train accuracy = 87.50%\n",
      "Step 2700, test loss = 0.51, test accuracy = 71.88%\n",
      "Step 2750, train loss = 0.30, train accuracy = 84.38%\n",
      "Step 2750, test loss = 0.41, test accuracy = 84.38%\n",
      "Step 2800, train loss = 0.24, train accuracy = 93.75%\n",
      "Step 2800, test loss = 0.57, test accuracy = 68.75%\n",
      "Step 2850, train loss = 0.42, train accuracy = 65.62%\n",
      "Step 2850, test loss = 0.50, test accuracy = 71.88%\n",
      "Step 2900, train loss = 0.58, train accuracy = 65.62%\n",
      "Step 2900, test loss = 0.48, test accuracy = 71.88%\n",
      "Step 2950, train loss = 0.26, train accuracy = 93.75%\n",
      "Step 2950, test loss = 0.48, test accuracy = 87.50%\n",
      "Step 3000, train loss = 0.41, train accuracy = 81.25%\n",
      "Step 3000, test loss = 0.27, test accuracy = 90.62%\n",
      "Step 3050, train loss = 0.47, train accuracy = 84.38%\n",
      "Step 3050, test loss = 0.57, test accuracy = 78.12%\n",
      "Step 3100, train loss = 0.23, train accuracy = 93.75%\n",
      "Step 3100, test loss = 0.28, test accuracy = 93.75%\n",
      "Step 3150, train loss = 0.51, train accuracy = 75.00%\n",
      "Step 3150, test loss = 0.41, test accuracy = 84.38%\n",
      "Step 3200, train loss = 0.42, train accuracy = 84.38%\n",
      "Step 3200, test loss = 0.33, test accuracy = 90.62%\n",
      "Step 3250, train loss = 0.28, train accuracy = 90.62%\n",
      "Step 3250, test loss = 0.28, test accuracy = 87.50%\n",
      "Step 3300, train loss = 0.38, train accuracy = 84.38%\n",
      "Step 3300, test loss = 0.25, test accuracy = 84.38%\n",
      "Step 3350, train loss = 0.41, train accuracy = 78.12%\n",
      "Step 3350, test loss = 0.54, test accuracy = 81.25%\n",
      "Step 3400, train loss = 0.23, train accuracy = 93.75%\n",
      "Step 3400, test loss = 0.22, test accuracy = 96.88%\n",
      "Step 3450, train loss = 0.28, train accuracy = 87.50%\n",
      "Step 3450, test loss = 0.51, test accuracy = 81.25%\n",
      "Step 3500, train loss = 0.31, train accuracy = 84.38%\n",
      "Step 3500, test loss = 0.36, test accuracy = 87.50%\n",
      "Step 3550, train loss = 0.30, train accuracy = 84.38%\n",
      "Step 3550, test loss = 0.65, test accuracy = 75.00%\n",
      "Step 3600, train loss = 0.44, train accuracy = 78.12%\n",
      "Step 3600, test loss = 0.27, test accuracy = 87.50%\n",
      "Step 3650, train loss = 0.21, train accuracy = 90.62%\n",
      "Step 3650, test loss = 0.40, test accuracy = 84.38%\n",
      "Step 3700, train loss = 0.21, train accuracy = 93.75%\n",
      "Step 3700, test loss = 0.10, test accuracy = 100.00%\n",
      "Step 3750, train loss = 0.30, train accuracy = 81.25%\n",
      "Step 3750, test loss = 0.44, test accuracy = 78.12%\n",
      "Step 3800, train loss = 0.18, train accuracy = 96.88%\n",
      "Step 3800, test loss = 0.49, test accuracy = 78.12%\n",
      "Step 3850, train loss = 0.17, train accuracy = 96.88%\n",
      "Step 3850, test loss = 0.20, test accuracy = 90.62%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3900, train loss = 0.41, train accuracy = 84.38%\n",
      "Step 3900, test loss = 0.27, test accuracy = 90.62%\n",
      "Step 3950, train loss = 0.18, train accuracy = 93.75%\n",
      "Step 3950, test loss = 0.40, test accuracy = 87.50%\n",
      "Step 4000, train loss = 0.28, train accuracy = 87.50%\n",
      "Step 4000, test loss = 0.29, test accuracy = 87.50%\n",
      "Step 4050, train loss = 0.34, train accuracy = 84.38%\n",
      "Step 4050, test loss = 0.51, test accuracy = 75.00%\n",
      "Step 4100, train loss = 0.15, train accuracy = 96.88%\n",
      "Step 4100, test loss = 0.46, test accuracy = 78.12%\n",
      "Step 4150, train loss = 0.28, train accuracy = 81.25%\n",
      "Step 4150, test loss = 0.37, test accuracy = 84.38%\n",
      "Step 4200, train loss = 0.29, train accuracy = 90.62%\n",
      "Step 4200, test loss = 0.33, test accuracy = 81.25%\n",
      "Step 4250, train loss = 0.41, train accuracy = 81.25%\n",
      "Step 4250, test loss = 0.49, test accuracy = 75.00%\n",
      "Step 4300, train loss = 0.34, train accuracy = 84.38%\n",
      "Step 4300, test loss = 0.26, test accuracy = 87.50%\n",
      "Step 4350, train loss = 0.14, train accuracy = 100.00%\n",
      "Step 4350, test loss = 0.22, test accuracy = 93.75%\n",
      "Step 4400, train loss = 0.20, train accuracy = 90.62%\n",
      "Step 4400, test loss = 0.59, test accuracy = 68.75%\n",
      "Step 4450, train loss = 0.18, train accuracy = 93.75%\n",
      "Step 4450, test loss = 0.34, test accuracy = 84.38%\n",
      "Step 4500, train loss = 0.11, train accuracy = 96.88%\n",
      "Step 4500, test loss = 0.13, test accuracy = 96.88%\n",
      "Step 4550, train loss = 0.24, train accuracy = 87.50%\n",
      "Step 4550, test loss = 0.40, test accuracy = 78.12%\n",
      "Step 4600, train loss = 0.14, train accuracy = 93.75%\n",
      "Step 4600, test loss = 0.27, test accuracy = 87.50%\n",
      "Step 4650, train loss = 0.09, train accuracy = 100.00%\n",
      "Step 4650, test loss = 0.41, test accuracy = 78.12%\n",
      "Step 4700, train loss = 0.26, train accuracy = 93.75%\n",
      "Step 4700, test loss = 0.28, test accuracy = 81.25%\n",
      "Step 4750, train loss = 0.24, train accuracy = 90.62%\n",
      "Step 4750, test loss = 0.66, test accuracy = 71.88%\n",
      "Step 4800, train loss = 0.17, train accuracy = 90.62%\n",
      "Step 4800, test loss = 0.45, test accuracy = 78.12%\n",
      "Step 4850, train loss = 0.14, train accuracy = 96.88%\n",
      "Step 4850, test loss = 0.28, test accuracy = 93.75%\n",
      "Step 4900, train loss = 0.29, train accuracy = 93.75%\n",
      "Step 4900, test loss = 0.57, test accuracy = 71.88%\n",
      "Step 4950, train loss = 0.27, train accuracy = 90.62%\n",
      "Step 4950, test loss = 0.55, test accuracy = 81.25%\n",
      "Step 5000, train loss = 0.16, train accuracy = 93.75%\n",
      "Step 5000, test loss = 0.38, test accuracy = 81.25%\n",
      "Step 5050, train loss = 0.04, train accuracy = 100.00%\n",
      "Step 5050, test loss = 0.67, test accuracy = 78.12%\n",
      "Step 5100, train loss = 0.10, train accuracy = 96.88%\n",
      "Step 5100, test loss = 0.35, test accuracy = 87.50%\n",
      "Step 5150, train loss = 0.19, train accuracy = 96.88%\n",
      "Step 5150, test loss = 0.12, test accuracy = 100.00%\n",
      "Step 5200, train loss = 0.19, train accuracy = 93.75%\n",
      "Step 5200, test loss = 0.51, test accuracy = 81.25%\n",
      "Step 5250, train loss = 0.12, train accuracy = 96.88%\n",
      "Step 5250, test loss = 0.52, test accuracy = 84.38%\n",
      "Step 5300, train loss = 0.18, train accuracy = 93.75%\n",
      "Step 5300, test loss = 0.66, test accuracy = 75.00%\n",
      "Step 5350, train loss = 0.11, train accuracy = 93.75%\n",
      "Step 5350, test loss = 0.36, test accuracy = 84.38%\n",
      "Step 5400, train loss = 0.08, train accuracy = 96.88%\n",
      "Step 5400, test loss = 0.59, test accuracy = 81.25%\n",
      "Step 5450, train loss = 0.13, train accuracy = 100.00%\n",
      "Step 5450, test loss = 0.42, test accuracy = 81.25%\n",
      "Step 5500, train loss = 0.23, train accuracy = 93.75%\n",
      "Step 5500, test loss = 0.51, test accuracy = 84.38%\n",
      "Step 5550, train loss = 0.31, train accuracy = 90.62%\n",
      "Step 5550, test loss = 0.41, test accuracy = 81.25%\n",
      "Step 5600, train loss = 0.12, train accuracy = 93.75%\n",
      "Step 5600, test loss = 0.29, test accuracy = 90.62%\n",
      "Step 5650, train loss = 0.06, train accuracy = 100.00%\n",
      "Step 5650, test loss = 0.60, test accuracy = 75.00%\n",
      "Step 5700, train loss = 0.10, train accuracy = 96.88%\n",
      "Step 5700, test loss = 0.66, test accuracy = 68.75%\n",
      "Step 5750, train loss = 0.12, train accuracy = 96.88%\n",
      "Step 5750, test loss = 0.24, test accuracy = 93.75%\n",
      "Step 5800, train loss = 0.13, train accuracy = 93.75%\n",
      "Step 5800, test loss = 0.56, test accuracy = 81.25%\n",
      "Step 5850, train loss = 0.14, train accuracy = 93.75%\n",
      "Step 5850, test loss = 0.43, test accuracy = 87.50%\n",
      "Step 5900, train loss = 0.15, train accuracy = 93.75%\n",
      "Step 5900, test loss = 0.54, test accuracy = 75.00%\n",
      "Step 5950, train loss = 0.12, train accuracy = 93.75%\n",
      "Step 5950, test loss = 0.22, test accuracy = 93.75%\n",
      "Step 6000, train loss = 0.21, train accuracy = 90.62%\n",
      "Step 6000, test loss = 0.70, test accuracy = 71.88%\n",
      "Step 6050, train loss = 0.20, train accuracy = 90.62%\n",
      "Step 6050, test loss = 0.66, test accuracy = 78.12%\n",
      "Step 6100, train loss = 0.05, train accuracy = 100.00%\n",
      "Step 6100, test loss = 0.52, test accuracy = 81.25%\n",
      "Step 6150, train loss = 0.15, train accuracy = 93.75%\n",
      "Step 6150, test loss = 0.42, test accuracy = 87.50%\n",
      "Step 6200, train loss = 0.18, train accuracy = 96.88%\n",
      "Step 6200, test loss = 0.34, test accuracy = 90.62%\n",
      "Step 6250, train loss = 0.07, train accuracy = 96.88%\n",
      "Step 6250, test loss = 0.53, test accuracy = 87.50%\n",
      "Step 6300, train loss = 0.07, train accuracy = 96.88%\n",
      "Step 6300, test loss = 0.22, test accuracy = 87.50%\n",
      "Step 6350, train loss = 0.14, train accuracy = 93.75%\n",
      "Step 6350, test loss = 0.69, test accuracy = 75.00%\n",
      "Step 6400, train loss = 0.10, train accuracy = 96.88%\n",
      "Step 6400, test loss = 1.12, test accuracy = 78.12%\n",
      "Step 6450, train loss = 0.03, train accuracy = 100.00%\n",
      "Step 6450, test loss = 0.30, test accuracy = 90.62%\n",
      "Step 6500, train loss = 0.07, train accuracy = 96.88%\n",
      "Step 6500, test loss = 0.30, test accuracy = 93.75%\n",
      "Step 6550, train loss = 0.04, train accuracy = 100.00%\n",
      "Step 6550, test loss = 0.32, test accuracy = 90.62%\n",
      "Step 6600, train loss = 0.11, train accuracy = 96.88%\n",
      "Step 6600, test loss = 0.36, test accuracy = 87.50%\n",
      "Step 6650, train loss = 0.02, train accuracy = 100.00%\n",
      "Step 6650, test loss = 0.95, test accuracy = 75.00%\n",
      "Step 6700, train loss = 0.11, train accuracy = 96.88%\n",
      "Step 6700, test loss = 0.40, test accuracy = 87.50%\n",
      "Step 6750, train loss = 0.07, train accuracy = 96.88%\n",
      "Step 6750, test loss = 0.50, test accuracy = 84.38%\n",
      "Step 6800, train loss = 0.03, train accuracy = 100.00%\n",
      "Step 6800, test loss = 0.42, test accuracy = 81.25%\n",
      "Step 6850, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 6850, test loss = 0.22, test accuracy = 90.62%\n",
      "Step 6900, train loss = 0.02, train accuracy = 100.00%\n",
      "Step 6900, test loss = 0.57, test accuracy = 81.25%\n",
      "Step 6950, train loss = 0.03, train accuracy = 100.00%\n",
      "Step 6950, test loss = 0.91, test accuracy = 75.00%\n",
      "Step 7000, train loss = 0.03, train accuracy = 100.00%\n",
      "Step 7000, test loss = 0.38, test accuracy = 87.50%\n",
      "Step 7050, train loss = 0.03, train accuracy = 100.00%\n",
      "Step 7050, test loss = 1.15, test accuracy = 75.00%\n",
      "Step 7100, train loss = 0.02, train accuracy = 100.00%\n",
      "Step 7100, test loss = 0.75, test accuracy = 84.38%\n",
      "Step 7150, train loss = 0.05, train accuracy = 96.88%\n",
      "Step 7150, test loss = 0.13, test accuracy = 93.75%\n",
      "Step 7200, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 7200, test loss = 0.45, test accuracy = 81.25%\n",
      "Step 7250, train loss = 0.03, train accuracy = 100.00%\n",
      "Step 7250, test loss = 0.29, test accuracy = 87.50%\n",
      "Step 7300, train loss = 0.03, train accuracy = 100.00%\n",
      "Step 7300, test loss = 0.79, test accuracy = 87.50%\n",
      "Step 7350, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 7350, test loss = 0.56, test accuracy = 81.25%\n",
      "Step 7400, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 7400, test loss = 0.81, test accuracy = 81.25%\n",
      "Step 7450, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 7450, test loss = 0.66, test accuracy = 87.50%\n",
      "Step 7500, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 7500, test loss = 0.89, test accuracy = 78.12%\n",
      "Step 7550, train loss = 0.17, train accuracy = 96.88%\n",
      "Step 7550, test loss = 1.22, test accuracy = 78.12%\n",
      "Step 7600, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 7600, test loss = 0.88, test accuracy = 81.25%\n",
      "Step 7650, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 7650, test loss = 1.03, test accuracy = 78.12%\n",
      "Step 7700, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 7700, test loss = 0.64, test accuracy = 75.00%\n",
      "Step 7750, train loss = 0.03, train accuracy = 100.00%\n",
      "Step 7750, test loss = 0.77, test accuracy = 78.12%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7800, train loss = 0.03, train accuracy = 100.00%\n",
      "Step 7800, test loss = 0.58, test accuracy = 84.38%\n",
      "Step 7850, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 7850, test loss = 0.85, test accuracy = 78.12%\n",
      "Step 7900, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 7900, test loss = 1.47, test accuracy = 71.88%\n",
      "Step 7950, train loss = 0.02, train accuracy = 100.00%\n",
      "Step 7950, test loss = 0.96, test accuracy = 78.12%\n",
      "Step 8000, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 8000, test loss = 0.64, test accuracy = 87.50%\n",
      "Step 8050, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 8050, test loss = 0.96, test accuracy = 87.50%\n",
      "Step 8100, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 8100, test loss = 0.67, test accuracy = 84.38%\n",
      "Step 8150, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 8150, test loss = 1.50, test accuracy = 75.00%\n",
      "Step 8200, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 8200, test loss = 1.10, test accuracy = 78.12%\n",
      "Step 8250, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 8250, test loss = 1.33, test accuracy = 78.12%\n",
      "Step 8300, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 8300, test loss = 0.82, test accuracy = 84.38%\n",
      "Step 8350, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 8350, test loss = 0.90, test accuracy = 84.38%\n",
      "Step 8400, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 8400, test loss = 1.27, test accuracy = 71.88%\n",
      "Step 8450, train loss = 0.02, train accuracy = 100.00%\n",
      "Step 8450, test loss = 1.16, test accuracy = 71.88%\n",
      "Step 8500, train loss = 0.02, train accuracy = 100.00%\n",
      "Step 8500, test loss = 1.01, test accuracy = 87.50%\n",
      "Step 8550, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 8550, test loss = 1.45, test accuracy = 75.00%\n",
      "Step 8600, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 8600, test loss = 0.32, test accuracy = 90.62%\n",
      "Step 8650, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 8650, test loss = 1.07, test accuracy = 84.38%\n",
      "Step 8700, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 8700, test loss = 1.10, test accuracy = 75.00%\n",
      "Step 8750, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 8750, test loss = 1.01, test accuracy = 81.25%\n",
      "Step 8800, train loss = 0.02, train accuracy = 100.00%\n",
      "Step 8800, test loss = 0.85, test accuracy = 81.25%\n",
      "Step 8850, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 8850, test loss = 1.16, test accuracy = 84.38%\n",
      "Step 8900, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 8900, test loss = 0.58, test accuracy = 81.25%\n",
      "Step 8950, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 8950, test loss = 0.61, test accuracy = 87.50%\n",
      "Step 9000, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 9000, test loss = 0.52, test accuracy = 84.38%\n",
      "Step 9050, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 9050, test loss = 0.54, test accuracy = 87.50%\n",
      "Step 9100, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 9100, test loss = 0.94, test accuracy = 84.38%\n",
      "Step 9150, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 9150, test loss = 0.81, test accuracy = 81.25%\n",
      "Step 9200, train loss = 0.02, train accuracy = 100.00%\n",
      "Step 9200, test loss = 1.61, test accuracy = 68.75%\n",
      "Step 9250, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 9250, test loss = 1.39, test accuracy = 62.50%\n",
      "Step 9300, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 9300, test loss = 0.92, test accuracy = 87.50%\n",
      "Step 9350, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 9350, test loss = 0.91, test accuracy = 87.50%\n",
      "Step 9400, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 9400, test loss = 0.49, test accuracy = 84.38%\n",
      "Step 9450, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 9450, test loss = 0.98, test accuracy = 81.25%\n",
      "Step 9500, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 9500, test loss = 0.52, test accuracy = 90.62%\n",
      "Step 9550, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 9550, test loss = 0.57, test accuracy = 87.50%\n",
      "Step 9600, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 9600, test loss = 0.71, test accuracy = 90.62%\n",
      "Step 9650, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 9650, test loss = 0.86, test accuracy = 87.50%\n",
      "Step 9700, train loss = 0.02, train accuracy = 100.00%\n",
      "Step 9700, test loss = 0.93, test accuracy = 75.00%\n",
      "Step 9750, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 9750, test loss = 0.41, test accuracy = 78.12%\n",
      "Step 9800, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 9800, test loss = 1.11, test accuracy = 75.00%\n",
      "Step 9850, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 9850, test loss = 0.47, test accuracy = 96.88%\n",
      "Step 9900, train loss = 0.00, train accuracy = 100.00%\n",
      "Step 9900, test loss = 1.06, test accuracy = 81.25%\n",
      "Step 9950, train loss = 0.01, train accuracy = 100.00%\n",
      "Step 9950, test loss = 1.35, test accuracy = 75.00%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    train_writer = tf.summary.FileWriter(logs_train_dir, sess.graph) \n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(init_op)\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    for step in np.arange(MAX_STEP):\n",
    "        if coord.should_stop():  #线程被要求停止则返回True\n",
    "            break   \n",
    "        img_batch1,label_batch1=sess.run([img_batch,label_batch])\n",
    "        img_batch_test1,label_batch_test1=sess.run([img_batch_test,label_batch_test])\n",
    "        sess.run(train_op,feed_dict={is_training: True,train_batch:img_batch1,train_label_batch:label_batch1,\n",
    "                                    val_batch:img_batch_test1,val_label_batch:label_batch_test1})\n",
    "        if step % 50  == 0:\n",
    "            tra_loss, tra_acc = sess.run([loss,acc],feed_dict={is_training:True,train_batch:img_batch1,train_label_batch:label_batch1,\n",
    "                                                              val_batch:img_batch_test1,val_label_batch:label_batch_test1})\n",
    "            tes_loss, tes_acc = sess.run([loss,acc],feed_dict={is_training:False,train_batch:img_batch1,train_label_batch:label_batch1,\n",
    "                                                              val_batch:img_batch_test1,val_label_batch:label_batch_test1})\n",
    "            print('Step %d, train loss = %.2f, train accuracy = %.2f%%' %(step, tra_loss, tra_acc*100.0))\n",
    "            print('Step %d, test loss = %.2f, test accuracy = %.2f%%' %(step, tes_loss, tes_acc*100.0))\n",
    "            \n",
    "            summary_str = sess.run(summary_op,feed_dict={is_training:False,train_batch:img_batch1,train_label_batch:label_batch1,\n",
    "                                                              val_batch:img_batch_test1,val_label_batch:label_batch_test1})\n",
    "            train_writer.add_summary(summary_str, step)\n",
    "        if (step + 1) == MAX_STEP:\n",
    "            checkpoint_path = os.path.join(logs_train_dir, 'model.ckpt')\n",
    "            saver.save(sess, checkpoint_path, global_step=step)\n",
    "#     tes_loss, tes_acc = sess.run([test_loss,test_acc])\n",
    "#     print('Step %d, test loss = %.2f, test accuracy = %.2f%%' %(step, tes_loss, tes_acc*100.0))\n",
    "    coord.request_stop()#请求该线程停止\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一次读一张图片，添加到列表中，由于内存有限，所以分几次读取 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from log/model.ckpt-9999\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test1/7001.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f73ce2ec3ccc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m#     train, train_label, val, val_label = get_files(train_dir, 0.1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m#     img = get_one_image(picture_dir)  #picture_dir为被测试图片的路径\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mevaluate_one_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f73ce2ec3ccc>\u001b[0m in \u001b[0;36mevaluate_one_image\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mpicture_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'test1/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0mimg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_one_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpicture_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mimg1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f73ce2ec3ccc>\u001b[0m in \u001b[0;36mget_one_image\u001b[0;34m(train)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mimg_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m   \u001b[0;31m#识别图片的路径\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mimag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#由于图片在预处理阶段以及resize，因此该命令可略\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2256\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2257\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2258\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2260\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test1/7001.jpg'"
     ]
    }
   ],
   "source": [
    "L=[]\n",
    "#七.读取一张图，用于识别\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#获取一张图片\n",
    "def get_one_image(train):\n",
    "    #输入参数：train,训练图片的路径\n",
    "    #返回参数：image，从训练图片中随机抽取一张图片\n",
    "    img_dir = train   #识别图片的路径\n",
    "\n",
    "    img = Image.open(img_dir)\n",
    "    plt.imshow(img)\n",
    "    imag = img.resize([128, 128])  #由于图片在预处理阶段以及resize，因此该命令可略\n",
    "    image = np.array(imag)\n",
    "    return image\n",
    "\n",
    "\n",
    "#八.测试图片\n",
    "\n",
    "def evaluate_one_image():\n",
    "    \n",
    "    with tf.Graph().as_default():\n",
    "        BATCH_SIZE = 1#识别一张图片\n",
    "        N_CLASSES = 2#类别\n",
    "        img = tf.placeholder(tf.float32,shape=[128,128,3])\n",
    "        image1 = tf.cast(img, tf.float32)\n",
    "        image2 = tf.image.resize_image_with_crop_or_pad(image1, 128, 128)\n",
    "        image3 = tf.image.per_image_standardization(image2)\n",
    "        image4 = tf.reshape(image3, [1,128, 128, 3])\n",
    "\n",
    "        logit1 = inference(image4, BATCH_SIZE, N_CLASSES,0,0)\n",
    "\n",
    "        logit = tf.nn.softmax(logit1)\n",
    "\n",
    "\n",
    "        # you need to change the directories to yours.\n",
    "        logs_train_dir = 'log' \n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            ckpt = tf.train.get_checkpoint_state(logs_train_dir)\n",
    "\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "\n",
    "                global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
    "\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "#                 print('Loading success, global_step is %s' % global_step)\n",
    "#             else:\n",
    "#                 print('No checkpoint file found')\n",
    "           #             saver.restore(sess,'/home/liu/new_start/make_data/123/net/my_net_rnn.ckpt')\n",
    "            \n",
    "#             for img_name in os.listdir('test1'):  #返回指定的文件夹包含的文件或文件夹的名字的列表\n",
    "\n",
    "            for i in range(7001,12501):\n",
    "                if i%100==0:\n",
    "                    print(i)\n",
    "                picture_dir = 'test1/'+str(i)+'.jpg'\n",
    "                img1 = get_one_image(picture_dir)\n",
    "        \n",
    "                prediction = sess.run(logit, feed_dict={img:img1})\n",
    "#                 print(prediction)\n",
    "                max_index = np.argmax(prediction)\n",
    "#                 print(max_index)\n",
    "                if max_index==1:\n",
    "#                 print('This is a dog with possibility %.6f' %prediction[:, 1])\n",
    "                    L.append(prediction[0][1])\n",
    "#             elif max_index==1:\n",
    "#                            print('This is a jiwawa with possibility %.6f' %prediction[:, 1])\n",
    "#             elif max_index==2:\n",
    "#                            print('This is a poodle with possibility %.6f' %prediction[:, 2])\n",
    "                else:\n",
    "#                 print('This is a cat with possibility %.6f' %prediction[:, 0])\n",
    "                    L.append(prediction[0][1])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#     train_dir = '/home/liu/new_start/test/inputdata'\n",
    "#     picture_dir = 'test1/20.jpg'\n",
    "#     picture_dir = '11111.jpeg'\n",
    "#     train, train_label, val, val_label = get_files(train_dir, 0.1)\n",
    "#     img = get_one_image(picture_dir)  #picture_dir为被测试图片的路径\n",
    "    evaluate_one_image()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('7001-12500.txt','w');\n",
    "\n",
    "file.write(str(L));\n",
    "\n",
    "file.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用于测试单张图片 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "#     train_dir = '/home/liu/new_start/test/inputdata'\n",
    "    picture_dir = 'test1/48.jpg'\n",
    "#     picture_dir = '11111.jpeg'\n",
    "#     train, train_label, val, val_label = get_files(train_dir, 0.1)\n",
    "    img = get_one_image(picture_dir)  #picture_dir为被测试图片的路径\n",
    "    evaluate_one_image(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
